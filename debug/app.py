#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PythonAnywhereç”¨Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰
ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°å‡ºåŠ›ã‚’å¼·åŒ–
"""

from flask import Flask, render_template, request, redirect, url_for, flash, jsonify, send_file
import os
import json
import csv
from datetime import datetime
import threading
import traceback

# ãƒ‡ãƒãƒƒã‚°ç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from crawler_web import WebCrawlerPythonAnywhere
    print("âœ… crawler_web ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ crawler_web ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    WebCrawlerPythonAnywhere = None

# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
app = Flask(__name__)

# PythonAnywhereç”¨è¨­å®š
app.config['SECRET_KEY'] = 'pythonanywhere-crawler-2024-secure-key'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB
app.config['DEBUG'] = False  # æœ¬ç•ªç’°å¢ƒã§ã¯False

# PythonAnywhereã®åˆ¶é™ã«å¯¾å¿œ
MAX_PAGES_LIMIT = 10  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã•ã‚‰ã«åˆ¶é™

# çµæœä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€
RESULTS_FOLDER = 'results'
if not os.path.exists(RESULTS_FOLDER):
    os.makedirs(RESULTS_FOLDER)
    print(f"âœ… {RESULTS_FOLDER} ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆé€²æ—ç®¡ç†ï¼‰
crawl_progress = {
    'is_running': False,
    'current_page': 0,
    'total_pages': 0,
    'percentage': 0,
    'status': 'å¾…æ©Ÿä¸­',
    'results': [],
    'start_time': None
}

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    print("âœ… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹")
    return render_template('index.html')

@app.route('/crawl', methods=['POST'])
def crawl():
    """ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹"""
    global crawl_progress
    
    try:
        print("ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡")
        
        url = request.form.get('url', '').strip()
        max_pages = int(request.form.get('max_pages', 10))
        
        print(f"ğŸ“ URL: {url}")
        print(f"ğŸ“ æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {max_pages}")
        
        if not url:
            flash('URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', 'error')
            return redirect(url_for('index'))
        
        # PythonAnywhereã®åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if max_pages > MAX_PAGES_LIMIT:
            flash(f'PythonAnywhereã®åˆ¶é™ã«ã‚ˆã‚Šã€æœ€å¤§{MAX_PAGES_LIMIT}ãƒšãƒ¼ã‚¸ã¾ã§ã§ã™', 'warning')
            max_pages = MAX_PAGES_LIMIT
        
        # é€²æ—ã‚’ãƒªã‚»ãƒƒãƒˆ
        crawl_progress = {
            'is_running': True,
            'current_page': 0,
            'total_pages': max_pages,
            'percentage': 0,
            'status': 'ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ä¸­...',
            'results': [],
            'start_time': datetime.now()
        }
        
        print("ğŸ”„ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹")
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
        thread = threading.Thread(target=crawl_background, args=(url, max_pages))
        thread.daemon = True
        thread.start()
        
        return redirect(url_for('progress'))
        
    except Exception as e:
        error_msg = f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ã‚¨ãƒ©ãƒ¼: {error_msg}")
        print(f"âŒ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        flash(error_msg, 'error')
        return redirect(url_for('index'))

def crawl_background(url, max_pages):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ"""
    global crawl_progress
    
    try:
        print(f"ğŸ”„ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹: {url}")
        
        if WebCrawlerPythonAnywhere is None:
            raise Exception("WebCrawlerPythonAnywhereã‚¯ãƒ©ã‚¹ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“")
        
        crawler = WebCrawlerPythonAnywhere()
        print("âœ… ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆæˆåŠŸ")
        
        def update_progress(current, total, status):
            crawl_progress['current_page'] = current
            crawl_progress['total_pages'] = total
            crawl_progress['percentage'] = int((current / total) * 100) if total > 0 else 0
            crawl_progress['status'] = status
            print(f"ğŸ“Š é€²æ—æ›´æ–°: {current}/{total} - {status}")
        
        print("ğŸ”„ ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œé–‹å§‹")
        results = crawler.crawl_website_with_progress(url, max_pages, update_progress)
        
        crawl_progress['results'] = results
        crawl_progress['is_running'] = False
        crawl_progress['status'] = f'å®Œäº†ï¼ {len(results)}ä»¶ã®ãƒšãƒ¼ã‚¸ã‚’åé›†ã—ã¾ã—ãŸ'
        
        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†: {len(results)}ä»¶ã®ãƒšãƒ¼ã‚¸ã‚’åé›†")
        
    except Exception as e:
        error_msg = f'ã‚¨ãƒ©ãƒ¼: {str(e)}'
        print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {error_msg}")
        print(f"âŒ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
        crawl_progress['is_running'] = False
        crawl_progress['status'] = error_msg

@app.route('/progress')
def progress():
    """é€²æ—è¡¨ç¤ºãƒšãƒ¼ã‚¸"""
    print("ğŸ“Š é€²æ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹")
    return render_template('progress.html')

@app.route('/api/progress')
def api_progress():
    """é€²æ—API"""
    print(f"ğŸ“¡ é€²æ—APIå‘¼ã³å‡ºã—: {crawl_progress}")
    return jsonify(crawl_progress)

@app.route('/results')
def show_results():
    """çµæœè¡¨ç¤ºãƒšãƒ¼ã‚¸"""
    global crawl_progress
    
    print("ğŸ“‹ çµæœãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹")
    
    if not crawl_progress['results']:
        flash('çµæœãŒã‚ã‚Šã¾ã›ã‚“', 'warning')
        return redirect(url_for('index'))
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_filename = f'crawl_results_{timestamp}.json'
    csv_filename = f'crawl_results_{timestamp}.csv'
    
    json_filepath = os.path.join(RESULTS_FOLDER, json_filename)
    csv_filepath = os.path.join(RESULTS_FOLDER, csv_filename)
    
    try:
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(crawl_progress['results'], f, ensure_ascii=False, indent=2)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(csv_filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['URL', 'Index Status', 'Title', 'H1', 'H2-1', 'H2-2', 'H2-3', 'Description', 'Canonical URL', 'Is Redirect', 'Redirect Chain', 'Final URL', 'Status Code'])
            for result in crawl_progress['results']:
                writer.writerow([
                    result['url'],
                    result['index_status'],
                    result['title'],
                    result['h1'],
                    result['h2_1'],
                    result['h2_2'],
                    result['h2_3'],
                    result['description'],
                    result['canonical_url'],
                    result['is_redirect'],
                    result['redirect_chain'],
                    result['final_url'],
                    result['status_code']
                ])
        
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {json_filename}, {csv_filename}")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    return render_template('results.html', 
                         results=crawl_progress['results'], 
                         json_filename=json_filename,
                         csv_filename=csv_filename,
                         total_count=len(crawl_progress['results']))

@app.route('/download/<filename>')
def download_file(filename):
    """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        file_path = os.path.join(RESULTS_FOLDER, filename)
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True)
        else:
            flash('ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“', 'error')
            return redirect(url_for('index'))
    except Exception as e:
        flash(f'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}', 'error')
        return redirect(url_for('index'))

if __name__ == '__main__':
    print("ğŸš€ Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
    app.run(debug=True)
